%%% Documento tipo para trabajos en LaTeX.
%%% Copyleft: Jesús Balsa, Juan F. García.

% Tipo de documento:
\documentclass[12pt,a4paper,onecolumn,oneside]{report}
\newcommand{\mychapter}[2]{
	\setcounter{chapter}{#1}
	\setcounter{section}{0}
	\chapter*{#2}
	\addcontentsline{toc}{chapter}{#2}
}

% Opcional: Tamaño personalizado para los márgenes:
\usepackage[a4paper, top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}
\usepackage{pifont}
\usepackage{float}
\usepackage{array}
\usepackage{amsmath}
\usepackage{lscape}
\usepackage{longtable}
\newcommand{\tabtitle}[1]{\cellcolor{CSGreen}\color{white}\textbf{#1}\rule{0pt}{20pt}}
\newcommand{\ltt}[1]{\cellcolor{CSGreen}\color{white}\textbf{#1}\rule{0pt}{8pt}}
\newcommand{\rtabtitle}[1]{\cellcolor{CSRed}\color{white}\textbf{#1}\rule{0pt}{20pt}}
\usepackage{multirow}
\usepackage[utf8]{inputenc} % Codificación UTF-8.
\usepackage[T1]{fontenc}    % Para usar caracteres con tilde.
\usepackage[spanish,es-tabla]{babel} % Escritura en castellano.
\usepackage{eurosym}  % Para el símbolo del EURO (€).
\usepackage{graphicx} % Paquete de imágenes, para introducir figuras.
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage[usenames,dvipsnames]{color} % Texto en colores.
\usepackage[table, RGB, usenames,dvipsnames]{xcolor}   % Extra colors.
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{lesslightblue}{rgb}{0.7,0.7,1.0}
\usepackage{listings}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstdefinelanguage{shell}
{
  backgroundcolor=\color{black},
  rulecolor=\color{codegray},
  basicstyle=\footnotesize\ttfamily\color{white},
  morecomment=[l]{\#},
  commentstyle=\footnotesize\ttfamily\color{lesslightblue}
}

\lstset{style=mystyle}

\renewcommand{\lstlistingname}{Script}
\usepackage{url}      % Para escritura de URLs.
\usepackage[breaklinks]{hyperref} % Hiperreferencias.
\usepackage{amsmath,amssymb} % Para los símbolos matemáticos.
\usepackage{cite}     % Para las citas de referencias (crea el superíndice).
\usepackage{listings} % Para coloreado de código fuente.
\usepackage{verbatim} % Para textos tipo consola y otros formatos.
\usepackage{fancyvrb} % Más opciones de verbatim.
\usepackage{parskip}  % OPCIONAL: Separa los párrafos con una línea en blanco.
\setlength{\parindent}{15pt} % Sangría de párrafos estándar (15 puntos). Necesario incluirla si se usa el paquete 'parskip'.
\usepackage[export]{adjustbox}
\usepackage{caption}  % Para personalizar los pies de foto.

% Opciones del paquete caption para los pies de imágenes y tablas:
\captionsetup{figurename=Figura, tablename=Tabla, labelsep=colon, labelfont=bf, font=small, justification=centering}

% Para el control de líneas viudas y huérfanas (líneas sueltas en páginas nuevas):
\usepackage[all]{nowidow}

\usepackage[nottoc]{tocbibind}    % Incluye el apartado "Referencias" en el índice.
%\def\spanishrefname{Bibliografía} % Para que ponga "Bibliografía" en lugar de "Referencias". SÓLO se aplica a formato "article". En "report" ya pone "Bibliografía".

\usepackage{fancyhdr}

\setlength{\unitlength}{1 cm} % Unidad de trabajo de medidas.

\renewcommand*{\baselinestretch}{1.25} % Altura del INTERLINEADO.
\renewcommand{\shorthandsspanish}{}    % Para que corte las palabras según el castellano.
\usepackage{makecell}

\usepackage[table, RGB, usenames,dvipsnames]{xcolor}   % Extra colors.
\usepackage{booktabs} % tablas con mejor estilo (toprule/midrule/bottomrule)
\usepackage{siunitx}  % formato numérico en tablas
\sisetup{group-separator = {,}, detect-all, round-mode=places, round-precision=2}

% Propiedades para el PDF generado (METADATOS):
\newcommand{\authorNames}{Aitor García Blanco}
\newcommand{\pdftitle}{Detección de células defectuosas en imágenes médicas}
\hypersetup{
  pdftitle={\pdftitle},% Título
  pdfauthor={\authorNames},% Autor
  pdfsubject={\pdftitle \ - \authorNames},% Asunto
% pdfkeywords={Opcional: algunas palabras clave}%
}

% Para la representación de código fuente:
% Para BASH:
\lstset{
	language=bash,
	basicstyle=\scriptsize,
	frame=single,
	numbers=left,
	numberstyle=\scriptsize,
	stepnumber=1,
	numbersep=9pt,
	backgroundcolor=\color{White},
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=4,
	captionpos=b,
	breaklines=true,
	keywordstyle=\color{blue}\bfseries,
	%identifierstyle=\color{green}\bfseries,
	stringstyle=\color{orange}\bfseries,
	commentstyle=\color{gray}\bfseries
}
% Para C++:
%\lstset{
%	language=C++,
%	frame=single,
%	keywordstyle=\color{Green}\bfseries,
%	identifierstyle=\color{BlueViolet},
%	stringstyle=\color{Red},
%	commentstyle=\color{MidnightBlue}
%}

\usepackage{fancyhdr} % Para el tamaño y estilo de los encabezados.

\fancypagestyle{headings}{% Redefine el estilo "headings".
	\fancyhf{} % Clear all header and footer fields.
	\lhead{\small \it Máster Universitario en Robótica e Inteligencia Artificial} 
	\rhead{\small \it Página \thepage}      % Nº de página a la derecha. Tamaño "small".
	\renewcommand{\headrulewidth}{1pt}
}

% Ajustes para división manual de palabras:
\hyphenation{Python} % Impide que la palabra Python sea dividida al acabar una línea.

\usepackage{titlesec}
\titleformat{\chapter}[hang]
  {\normalfont\huge\bfseries}
  {\thechapter.}{1em}{}

\titlespacing*{\chapter}{0pt}{0pt}{1em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INICIO DEL DOCUMENTO: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\chaptername}{}
\begin{document}

\input{Plantillas/Plantilla_Portada}

% Página de FIRMAS:
\newpage

\thispagestyle{empty} % para que no se numere esta página.

\input{Plantillas/Plantilla_Firmas}

\newpage
\pagestyle{plain}

\renewcommand{\thepage}{\roman{page}}
\setcounter{page}{1} % Esta página es la 1.

% Página con el ÍNDICE GENERAL
\renewcommand{\contentsname}{Índice}
\tableofcontents

% Página con el ÍNDICE DE FIGURAS
\listoffigures

% Página con el ÍNDICE DE TABLAS
\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                                            Glosario de Términos                                             %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Página con el GLOSARIO:
\mychapter{0}{Glosario de términos}
\label{chap:glosario}
% A partir de aquí ya se incluye el encabezado en las páginas:
\pagestyle{headings}

% Al ser un capítulo sin número, hay que indicarle qué título añadir al encabezado de la página:
\markboth{GLOSARIO}{} 

\begin{description}
  \item[\textit{Bounding Box}]: Rectangulo que delimita la posición y tamaño de un objeto en una imagen.
  \item[\textit{Dataset}]: Conjunto de datos estructurados, utilizado para entrenar, validar o evaluar modelos de Inteligencia Artificial.
  \item[IoU \textit{(Intersection over Union})]: Área de unión entre dos \textit{bounding boxes} (anotación y prediccion).
  \item[Métricas]: Indicadores cuantitativos para la evaluación de los modelos de Inteligencia Artificial.
  \item[Experto]: Persona con conocimientos especializados en un dominio específico (como biólogos, biotecnólogos, patólogos, etc)
  \item[Detección de Objetos]: Tarea de localizar y clasificar instancias de objetos en imágenes mediante \textit{bounding boxes} y etiquetas.
  \item[NMS (\textit{Non-Maximum Suppression)}]: Algoritmo para eliminar las detecciones redundantes, manteniendo la caja con mayor confianza entre las solapadas
  \item[Aprendizaje profundo o \textit{deep learning}]: Área del aprendizaje automático que emplea redes neuronales profundas para aprender representaciones y resolver tareas complejas.
  \item[\textit{Transfer Learning}]: Reutilizar un modelo preentrenado y adaptarlo para entrenar un nuevo modelo con el conocimiento ya adquirido previamente con un dataset generalmente mayor.
  \item[\textit{framework}]: Conjunto estructurado de herramientas, librerías y convenciones que facilita y agiliza el desarrollo de aplicaciones o sistemas.
  % \item[\textit{Ensemble}]: Técnica para la unificación de predicciones de diferentes modelos.
  % \item[\textit{Cross Validatio}]: Técnica estadística para la evaluación de modelos, que se fundamenta en dividir el dataset en multiples pliegues (\textit{folds}) para entrenar y validar el modelo de forma iteractiva.
  % \item[\textit{Data augmentation}]: Conjunto de transformaciones sobre los datos como rotaciones, traslaciones, adición de ruido, entro otros, con el objetivo de aumentar la diversidad del dattaset y mejorar la generalización del modelo.
  % \item[Optuna]: Biblioteca de Python para la optimización de hiperparámetros, permitiendo encontrar las mejores configuraciones de estos. 
  \item[Streamlit]: Framework de Python para crear applicaciones web interactivas orientadas a la ciencia de datos y \textit{Machine Learning}.
  \item[Monolítica]: Arquitectura de software en la que la aplicación se despliega y ejecuta en un mismo proceso.     
  \item[LabelImg]: Herramienta gráfica de código abierto para la anotación de imágenes con \textit{bounding boxes}.
  \item[\textit{Widget}]: Elemento de una interface interactiva que permite al usuario controlar el comportamiento de una aplicación.
  \item[\textit{Ground truth}]: Hace alusión a las anotaciones correctas que describen las \textit{bounding boxes} de las imágenes.
  \item[\textit{Checkbox}]: Es un \textit{widget} que permite al usuario activar o desactivar una opción (boleano).  
  \item[\textit{linter}]:  Herramienta que analiza automáticamente el código fuente para detectar errores de sintaxis u otros posibles fallos antes de ejecutar el programa. 
  \item[Células redondas]: Según la OMS \cite{OMS}, toda célula no espermática que se encuentra en el eyaculado. Agrupa principalmente dos típos: células germinales inmaduras y leucocitos.
  \item[Células germinales inmaduras]: Células precursoras de los espermatozoides que no han completado su desarrollo.
  \item[Leucocitos]: Glóbulos blancos, principalmente netrófilos.  
  \item[Espermograma]: Consiste en analizar los espermatozoides de un hombre o un animal para evaluar la fertilidad y detectar posibles anomalías. 
  \item[Espermatogénesis]: Proceso biológico de formación y producción de espermatozoides.
  \item[Andrología]: Área de la medicina que se centra en el estudio e investigación de cualquier aspecto relacionado con la función sexual y reproducción masculina (Por ejmplo: infertilidad masculina).  
  \item[Anillos de Newton]: Interferencia de la luz cuyo resultado es un patrón de anillos muy característico.
  \item[\textit{overfitting}] Proceso en el que un modelo de aprendizaje automático aprende demasiado bien los datos de entrenamiento, perdiendo capacidad de generalización y obteniendo peores resultados en las pruebas de evaluación.
\end{description} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                                           Inicio de los CAPÍTULOS                                           %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1} % Esta página es la 1.

\chapter{Introducción y objetivos} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Introducción y objetivos}
\section{Introducción}
\label{sec:Introducción}

El presente Trabajo de Fin de Máster, desarrollado en colaboración con la empresa Microptic S.L \cite{microptic}, compañía de Hamilton Thorne \cite{HamiltonThorneWeb}, 
parte del desarrollo de una Prueba de Concepto (\textit{Proof of Concept}) desarrollada en el marco europeo DIGIS3 \cite{digis3} para la detección de células redondas en imágenes médicas 
mediante Inteligencia Artificial. Este trabajo incluye, además, la creación de una interfaz digital que permite la interacción de expertos 
con los resultados del modelo.

El objetivo de esta solución es la identificación automatizada de células redondas en imágenes de muestras de semen humano. Esta capacidad es de 
gran interés para Microptic \cite{microptic}, ya que optimiza los tiempos de análisis de los expertos y enriquece el estudio de las muestras espermáticas, 
mejorando la caracterización de parámetros clave tanto en el ámbito de la reproducción asistida humana como en la investigación y producción animal. 

La metodología se ha centrado, en primer lugar, en la consolidación de un conjunto de datos de alta calidad. Para ello, se procesó y validó el feedback 
proporcionado por los expertos de Microptic \cite{microptic}, refinando el etiquetado de las imágenes que constituyen la base para la evaluación de los modelos.

Posteriormente, aplicando técnicas de Aprendizaje Profundo (\textit{Deep Learning}) y Visión por Computador, se adaptaron y reentrenaron varias versiones 
del modelo de detección de objetos en tiempo real YOLO \cite{ultralytics_models}. El propósito fue especializar su capacidad, pasando de la detección de objetos 
cotidianos a la identificación precisa de células redondas para facilitar el espermograma. Finalmente, el modelo fue entrenado y evaluado 
para validar su rendimiento, eficacia y viabilidad.

De este modo, el proyecto trasciende la investigación académica para dar continuidad la misión de DIGIS3 \cite{digis3}: "impulsar la digitalización de las empresas a través 
de soluciones tecnológicas avanzadas". La herramienta web desarrollada actúa como un catalizador de la transformación digital para Microptic \cite{microptic}, traduciendo 
un complejo modelo de Inteligencia Artificial en una solución práctica que potencia sus capacidades de análisis y establece un precedente para futuras innovaciones.

\section{Objetivos}
\label{sec:Objetivos}

Este Trabajo de Fin de Máster tiene como objetivo principal el diseño, desarrollo y validación de un sistema inteligente para la detección automatizada de células redondas en imágenes médicas de muestras de semen humano;
mediante el empleo de técnicas de visión por computador y aprendizaje profundo. 
Demostrando así, la integración de competencias técnicas avanzadas en inteligencia artificial, procesamiento de imágenes y desarrollo de software, entre otras habilidades transversales. 

Con este proyecto, se pretende abordar los siguientes objetivos específicos: 

\begin{itemize}
  \item Investigar y estudiar casuísticas análogas en la detección de células en conjuntos de imágenes médicas.
  \item Rrealizar un preprocesamiento y anotación de los conjuntos de imágenes médicas.
  \item Implementar, entrenar y validar diferentes modelos u arquitecturas de modelos, comparar su rendimiento y optimizar hiperparámetros.
  \item Evaluar cuantitativamente los modelos mediante métricas estandar.
  \item Desarrollar una herramienta web interactiva para visualización, inferencia y exportación de resultados que facilite la validación por parte de expertos biomédicos.
  \item Garantizar la reproducibilidad y cumplimiento ético/legislativo.
  \item Documentar exhaustivamente el proyecto.
\end{itemize}


\chapter{Antecedentes} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Antecedentes}

En este capítulo se expone el problema abordado en este trabajo, sus antecedentes e hipótesis y motivaciones. 
Asimismo, se revisa la literatura existente para situar el estado del arte y justificar la necesidad de la investigación realizada.

\section{Contexto y motivación}
\label{sec:Contexto y motivación}

El análisis de muestras de semen es un pilar fundamental en el campo de la medicina reproductiva. Su relevancia abarca desde el diagnóstico 
de la infertilidad en parejas y técnicas de reproducción asistida en humanos, hasta la investigación y producción en el sector ganadero. Tradicionalmente,
este análisis se ha centrado en la motilidad, concentración y morfologia de los espermatozoides.

Sin embargo, un análisis seminal requiere la evaluación de otros componentes celulares, entre los que destacan las células redondas. 
Estas constituyen un indicador diagnóstico crucial, ya que abarca linfocitos, células plasmáticas, macrófagos, mastocitos y células tumorales redondas; cuya presencia puede ser signo de infecciones o inflamaciones 
del tracto genital, como células germinales inmaduras, que pueden indicar una alteración en el proceso de espermatogénesis \cite{HamiltonThorneRoundCells}.

El método estandar para la identificación y conteo de células redondas depende de la agudeza visual de un experto a través de un microscopio; lo que convierte al proceso de
espermatogenia es un proceso subjetivo (depende del observador), lento, laborioso y cansado para el observador. Estas limitaciones motivan la necesidad 
de desarrollar alternativas mediante herramientas automatizadas que permitan obtener resultados más rápidos, objetivos y reproducibles, optimizando el espermograma.

El auge de la Inteligencia Artificial, y en particular las técnicas de Visión por Computador basadas en Aprendizaje Profundo, están protagonizando una revolución dentro del mundo de la medicina.
La capacidad de las redes neuronales en la detección de patrones complejos han demostrado ser una herramienta de apoyo en diversas especialidades como son: análisis de imágenes radiológicas o análisis de lesiones cutáneas en dermatología, entre otras.
En \textit{computer vision}, modelos de detección de objetos como YOLO (You Only Look Once) \cite{ultralytics_models} han demostrado un equilibrio entre precisión y velocidad de inferencia. Estos algoritmos son capaces de procesar imágenes y localizar multiples objetos de interes en milisegundos.
Siendo idóneo en este tipo de trabajos clínicos.

Atendiendo a estas necesidades y casuísticas, por parte de una empresa lider en el análisis espermático, surge de la necesidad de integrar estos sistemas inteligentes para la detección de células redondas al que dió solución el grupo GVIS de la 
Universidad de León mediante una prueba de concepto \cite{HamiltonThorneRoundCells}. Demostrando así, la capacidad de estas tecnologías en este tipo de tareas complejas.

Aprovechando este hito, surge la motivación de definir una nueva prueba de concepto integrada en un entorno digital, que permita igualar o superar con menos recursos, la anterior prueba de concepto. Esto se aborda a través de diferentes vías:

\begin{itemize}
  \item Explorar diferentes arquitecturas de YOLO \cite{ultralytics_models}, desde modelos oficiales hasta prototipos más recientes que emplean capas de atención. 
  \item Diseñar un modelo peronalizado para competir con los modelos existentes.
  \item Optimización y mejora de la detección: buscar la configuración optima que maximice diferentes métricas y abordar la detección de artefactos visuales que fueron identificados como una limitación en la PoC previa. 
  \item Desarrollar de una herramienta web interactiva que no solo sirva para evalauar imágenes sino que además, de soporte a expertos.
\end{itemize}

\section{Estado del arte e hipótesis}
\label{sec:Estado del arte e hipótesis}

En esta sección se analiza el panorama tecnológico que sirve como fundamento para este proyecto. Se parte de las tecnologías de Inteligencia artificial que tienen relevancia en el campo de la medicina como es la 
visión por computador, se abordan los modelos de detección de objetos utilizados y se revisan las diferentes técnicas de optimización. Finalmente, se realiza un breve y conciso estudio sobre la literatura actual 
en este campo y se define la hipótesis de trabajo.

\subsection{Visión por computador y modelos de detección de objetos}

La Inteligencia Artificial está en auge y algunos de los campos en los que está tomando protagonismo son el Apendizaje Profundo y la Visión por Computador; campos en los que se fundamenta esta prueba de concepto.
La Visión por Computador está revolucionando el análisis de imágenes dadas las capacidades de estos sistemas de interpretar datos visuales en tareas como la clasificación, segmentación semántica o la detección de objetos en la que se fundamenta este proyecto.
Que fundamentadas en Aprendizaje Profundo, mediante el uso de Redes Neuronales, los algoritmos pueden identificar patrones complejos en las imágenes con una precisión igual o superior a la del hojo humano. 

Dentro de este campo, la detección de objetos tiene como objeto localizar espacialmente (mediante \textit{bounding boxes}) y clasificar cada úna de las múltiples instancias de un objeto de interés dentro de una misma imágen.
Esta tarea es crucial para este proyecto, pues va a permitir detectar y contar las células redondas en una imagen de esperma humano.

La familia de modelos de YOLO (You Only Look Once) \cite{ultralytics_models} se ha consolidado como un referente para tareas de detección, permitiendo un equilibrio entre precisión y velocidad.
Dentro de las 12 arquitecturas de YOLO \cite{ultralytics_models}, se emplean las siguientes \cite{defyolos}:

\begin{itemize}
  \item \textbf{Yolov8:} Adopta un diseño eficiente y sin anclajes (anchor-free). Su arquitectura utiliza una versión refinada de módulos basados en CSP y una cabeza desacoplada que separa las tareas de clasificación y regresión, logrando un buen equilibrio entre velocidad y precisión \cite{defyolos}.
  \item \textbf{Yolov9:} Introduce la Información de Gradiente Programable (PGI) y la Red de Agregación de Capas Generalizada y Eficiente (GELAN). Estas innovaciones ayudan a mantener gradientes robustos a través de la red, mejorando la convergencia y la capacidad para detectar objetos pequeños al evitar la pérdida de información \cite{defyolos}.
  \item \textbf{Yolov10:} Elimina la necesidad de la supresión no máxima (NMS), un paso de postprocesamiento que consume tiempo, mediante una novedosa estrategia de asignación dual. Esto reduce significativamente la latencia de inferencia y lo hace más eficiente para la detección en tiempo real \cite{defyolos}.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figuras/arquitecture YOLO/v8_v9_v10.png}
  \caption{Diagramas de arquitectura de Yolov8, Yolov9 y Yolov10}
  \label{fig:Yolov8_v9_v10_arquitectures}
  (a) Yolov8 presenta una una estructura principal basada en CSP, un cabezal desacoplado y libre de anclajes (anchor-free), y una red de pirámide de características (FPN) optimizada para una extracción eficiente de características a múltiples escalas. 
  (b) Yolov9 integra la Información de Gradiente Programable con GELAN para una agregación robusta de características.
  (c) Yolov10 presenta una estrategia de asignación dual, cabezales ligeros y un submuestreo desacoplado de canal espacial para mejorar la precisión y velocidad de la inferencia.
\end{figure}

\begin{itemize}
  \item \textbf{Yolov11:} Se enfoca en mejorar la extracción de características mediante el bloque C3k2, el módulo Spatial Pyramid Pooling-Fast (SPPF) y la atención espacial paralela (C2PSA). Estas mejoras le permiten un rendimiento más preciso, especialmente en escenarios con objetos ocluidos \cite{defyolos}.
  \item \textbf{Yolov12:} Su principal innovación es el módulo de Atención de Área (A²), que ajusta dinámicamente el campo receptivo para capturar señales contextuales globales y locales con un costo computacional mínimo. Esto, junto con la red R-ELAN, optimiza la fusión de características y aumenta drásticamente la velocidad de inferencia \cite{defyolos}.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{figuras/arquitecture YOLO/v11_v12.png}
  \caption{Diagramas de arquitectura de Yolov11 y Yolov12}
  \label{fig:Yolov11_v12_arquitectures}
  (a) YOLOv11 utiliza una arquitectura mejorada que incluye bloques C3k2, SPPF y C2PSA. Estos componentes optimizan la extracción de características en diversas escalas y mejoran la atención espacial, lo que se traduce en una mayor precisión de detección.
  (b) YOLOv12 avanza sobre este diseño con una arquitectura centrada en la atención. Al integrar módulos de Area Attention y bloques R-ELAN, optimiza la combinación de características, logrando un aumento drástico en la velocidad de inferencia para una detección de objetos en tiempo real de vanguardia.
\end{figure}

\subsection{Técnicas de optimización y validación de modelos}

Es bien sabido de la existencia de diferentes tecnologías para asegurar la robustez y maximizar el rendimiento de los modelos. Algunas de estas tecnologías, son las siguientes:

\begin{itemize}
  \item \textbf{Optimización de hiperparámetros (Optuna):} El \textit{framework} Optuna \cite{optuna_github}, de optimización automática de hiperparámetros (tales como la tasa de aprendizaje, el \textit{momentum} o regulación) explora de manera inteligente y eficiente el espacio de búsqueda y, al final, devuelve la mejor combinación de hiperparámetros encontrada. Se fundamenta en algoritmos de muestreo bayesiano y mecanismos de poda (\textit{pruning}) que descartan de manera anticipada los ensayos poco prometedores.
  \item \textbf{Aumento de datos (\textit{data augmentation}):} Para mejorar la capacidad de generalización de un modelo y reducir el riesgo de sobreajuste (\textit{overfitting}) en conjuntos de datos limitados se recurre a esta técnica. esta técnica permite generar nuevas muestras de entrenamiento aplicando transformaciones geométricas (rotaciones, traslaciones, escalado, etc) y fotométricas (cambios de brillo, contraste, etc) sobre las imágenes originales, incrementando así la diversidad del conjunto de datos.
  \item \textbf{Valización cruzada (\textit{cross validation}):} Dentro de sus múltiples variantes, la más común consiste en dividir el conjunto de datos en "k" subconjuntos o pliegues. El modelo se entrena K veces, utilizando en cada iteración un pliegue diferente para la validación y los K-1 restantes para el entrenamiento. El rendimiento final se calcula como la media de los resultados de las K iteraciones, proporcionando una medida de la capacidad de generalización del modelo menos dependiente de una única división de datos. Esta técnica permite conocer si un modelo es fiable y robusto.
  \item \textbf{Ensamblado de modelos (\textit{ensemble}):} Esta técnica avanzada busca mejorar la precisión y la robustez de las predicciones combinando las salidas de múltiples modelos entrenados de forma independiente. La hipótesis subyacente es que los errores de un modelo pueden ser compensados por los aciertos de otros (al promediar o dar pesos sobre las predicciones individuales), especialmente si los modelos son diversos.
\end{itemize}

\subsection{Literatura de la investigación}

La Inteligencia Artificial (IA), y en particular el Aprendizaje Profundo (\textit{Deep Learning}), están transformando el campo de análisis de imágenes médicas en especialidades como la oncología, la neurología y la oftalmología.

En el ámbito de la andrología, la Inteligencia Artificial está comenzando a tomar especial relevancia en tareas como la concentración y recuento de espermatozoides, motilidad espermática, morfología espermática, integridad del ADN espermático, entre otras \cite{PannerSelvam}. En el contexto del análisis seminal, la mayor parte de las investigaciones se centran
en la automatización del análisis morfológico de los espermatozoides, desarrollando modelos capaces de identificar con alta precisión diferentes partes de la estructura celilar (como cabeza, parte intermedia y cola) \cite{Maalej2025}.

Sin embargo, este proyecto presenta un desafío diferente y menos explorado, que a diferencia del análisis morfológico el cual se centra en un único tipo de célula,
la detección de células redondas, como ya hemso visto anteriormente, implicaidentificar un grupo heterogéneo de células (principalmente leucocitos y células germinales inmaduras) en un entornocomplejo y "ruidoso" \cite{OMS}\cite{BJBS}.
La dificultad de identificar estas células redondas, incluso para experos, y la subjetividad de los metodos manuales tradicionales \cite{Johanisson2000} justifican la necesidad de desarrollar herramientas automatizadas.

Este Trabajo de Fin de Máster se fundamenta en una innovadora prueba de concepto previa, desarrollada por el grupo de investigación GVIS de la Universidad de León en colaboración con la empresa Microptic S.L. \cite{microptic}, en el marco del proyecto europeo DIGIS3 \cite{digis3}. 
Dicha prueba de concepto, tuvo como objetivo validar la viabilidad de utilizar modelos de Deep Learning para la identificación de células redondas en imágenes de semen humano. En ese trabajo inicial, se reentrenaron tres variantes del modelo YOLOv7 utilizando un conjunto de 
alrededor de 400 imágenes, logrando resultados prometedores en diferentes métricas. Este hito demostró que los detectores de objetos en tiempo real podían ser adaptados con éxito a esta tarea específica a pesar de las dificultades que esto supone.

Este hito sirve de precedente para explorar nuevas y mejoradas arquitecturas para superar las métricas obtenidas. Investigaciones de vanguardia han desarrollado modelos de alta precisión para identificar específicamente espermátidas redondas humanas, 
aunque utilizando muestras purificadas mediante citometría de flujo, lo que simplifica el problema al eliminar el "ruido" de fondo \cite{roundsCellsSpermatid}.

Por otro lado, trabajos como el modelo ACTIVE abordan un escenario más similar al de este proyecto, al detectar espermatozoides e "impurezas" en videos microscópicos sin procesar \cite{chen2024}. Los resultados para al detección de espermatozoides son realmente buenos, sin embargo, no se puede decir lo mismo
para las impurezas (todos los objetos presentes en la muestra que no son espermatozoides). Los propios autores admiten que, en algunos casos, incluso para expertos es dificil distinguir si un objeto
es una impureza o un espermatozoide. Esto introduce ambigüedad en los datos de entrenamiento perjudicando el aprendizaje del modelo, especialmente para la clase más dificil de definir ("impurezas") \cite{chen2024}.

En síntesis, este proyecto da continuidad al desarrollo tecnológico a partir de la prueba de concepto inicial a demás de integrar y buscar superar los desafios identificados en la literatura científica más reciente. La prueba de concepto sirve como precedente para dar continuidad y actualizar las 
arquitecturas para el problema de la detección de células redondas en muestras de semen humano, mientras que la literatura reciente pone de manifiesto los desafios y dificultades en la detección de células heterogeneas. 


\subsection{Hipótesis de trabajo}

Considerando el potencial de las arquitecturas YOLO y las diferentes técnicas de optimización, se considera que mediante la exploración de las arquitecturas YOLO que van desde la versión 8 a la 12, 
junto con el diseño personalizado y la optima optimización de hiperparámetros, se consiga igualar o mejorar los modelos de la anterior prueba de concepto del grupo GVIS, 
en las méticas mAP y velocidad de inferencia. Adicionalmente, se espera que gracias a la arquitectura en desarrollo de Yolov12 que incorpora mecanismos de atención, el algoritmo no identifique 
como células redondas a los Anillos de Newton. La integración de estos modelos con la herramienta web sirvan no solo como metodo de validación final, sino que también demuestre la viabilidad del sistema 
para ser utilizado en un marco clínico de análisis real por personal experto.

\section{Definición del problema}
\label{sec:Definición del problema}

La problemática que da origen a este proyecto no es otra que la ardua y extenuante tarea que le supone a un expero la revisión manual de imágenes médicas de muestras de semen.

Por esta razón, el problema central que aborda este proyecto es diseñar, desarrollar y evaluar un sistema inteligente capaz de automatizar la detección de células 
redondas sobre un conjunto de imágenes médicas de muestras de semen, utilizando para ello, tecnicas de Visión por Computador (\textit{Computer Vision}) y Aprendizaje Profundo (\textit{Deep Learning}).

Este sistema inteligente debe ser capaz de pocesar imágenes microscópicas como las de la \autoref{fig:Muestra de semen humano sin anotaciones}, donde las células objetivo cohexisten con una densa población de espermatozoides, diversos artefactos visuales (Anillos de Newton)
y restos residuales ajenos al análisis. Asimismo, la dificultad que supone la distinción de células redondas de otros elementos morfológicamente similares (espermatozoides sin cola), esferas de las que se desconoce que son, etc.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figuras/rounds_cells/61.jpg}
  \caption{Muestra de semen humano sin anotaciones}
  \label{fig:Muestra de semen humano sin anotaciones}
\end{figure}

Para más inri, la asignación de anotaciones sobre el conjunto de imágenes es un problema que va a generar complicaciones durante todo el proyecto. Por ejemplo,
en la imagen \ref{fig:Anillos_Newton} anotada por un experto, presenta la problemática de los Anillos de Newton. Se verá posteriormente como afectan estos objetos a la correcta identificación de células redondas.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figuras/rounds_cells/Anillos de Newton.png}
  \caption{Ilustración con Anillos de Newton.}
  \label{fig:Anillos_Newton}
  Las \textit{bounding boxes} corresponden con las células redondas y los círculos rojos con los Anillos de Newton.
\end{figure}

Asimismo, en la imagen \ref{fig:feedback_experto} se evidencian las constantes dudas a cerca de las anotaciones de células redondas, no solo por el personal que define el proyecto sino por los propios expertos de Microptic \cite{microptic}. 
Cuyo \textit{feedback} fue:

\begin{quote}
“La mayoría de células marcadas con los rectángulos verdes son ciertas células redondas. Los círculos rosas también serían células (probablemente germinales, pero se necesitaría hacer una evaluación morfológica específica para saberlo). Los círculos azules marcan correctamente algunas células redondas pero también indican espermatozoides que retienen la mayor parte del citoplasma (esferas grises con un área blanca bien contrastada). En cuanto a los círculos naranjas, muchos son cabezas de espermatozoides sin cola y las esferas más pequeñas desconocemos qué son.”
\end{quote}

Esto evidencia, una vez más, el alto grado de complejidad que supone anotar este tipo de imágenes médicas, incluso bajo el marco de recomendadiones de la OMS \cite{OMS} \cite{BJBS}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figuras/rounds_cells/feedback_experto.png}
  \caption{Ilustración de annotaciones para \textit{feedback} con experto.}
  \label{fig:feedback_experto}
\end{figure}

\chapter{Gestión de proyecto software} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Gestión de proyecto software}

\section{Alcance del proyecto}
\label{Alcance del proyecto}

\subsection{Definición del proyecto}

\subsection{Presupuesto}

\subsubsection{Coste de personal}

\subsubsection{Coste del hardware}

\subsubsection{Costes indirectos}

\subsubsection{Coste total}

\section{Plan de trabajo}
\label{Plan de trabajo}

\subsection{Metodología}
\label{Metodología}

\subsection{Identificación de tareas}
\label{Identificación de tareas}

\subsection{Estimación de tareas}
\label{Estimación de tareas}

\subsection{Planificación de tareas}
\label{Planificación de tareas}

\section{Gestión de recursos}
\label{Gestión de recursos}

\subsection{Especificación y asignación de recursos}
\label{Especificación y asignación de recursos}

\section{Gestión de riesgos}
\label{sec:Gestión de riesgos}
\subsection{Identificación y análisis de riesgos}

\clearpage
\section{Legislación y normativa}
\label{Legislación y normativa}

En el marco de ejecución de este proyecto, se ha llevado a cabo un riguroso cumplimiento de la legislación y normativa vigente. A continuación, 
se detalla cómo el proyecto se ajusta y adhiere a las leyes pertinentes:

\begin{itemize}
    \item \textbf{Ley Orgánica 3/2018, de 5 de diciembre, de Protección de Datos Personales y garantía de los derechos digitales}\cite{LOPD2018}
    
    Este proyecto respeta plenamente la Ley Orgánica 3/2018, la cual reconoce el derecho fundamental a la protección de datos personales. 
    La creación y tratamiento del dataset de imágenes médicas se ha realizado conforme a las disposiciones de la ley, asegurando la legalidad en el tratamiento de datos biomédicos. 
    Se han obtenido los permisos explícitos necesarios para el uso de las imágenes, garantizando la privacidad y anonimización de los datos de los pacientes. 
    
    \item \textbf{Reglamento (UE) 2016/679 del Parlamento Europeo y del Consejo relativo a la protección de las personas físicas en lo que respecta al tratamiento de datos personales y a la libre circulación de estos datos (RGPD)}\cite{RGPD2016}

    La creación y tratamiento del dataset de imágenes médicas en este proyecto, se ha realizado conforme a los principios del RGPD, garantizando la legalidad, transparencia en el 
    tratamiento de datos biomédicos. Todas las imágenes han sido previamente anonimizadas y se han implementado medidas técnicas y organizativas 
    para asegurar la seguridad y privacidad de los datos, cumpliendo así con las exigencias del RGPD para datos de salud considerados de categoría 
    especial.
    
    \item \textbf{Reglamento (UE) 2024/1689 del Parlamento Europeo y del Consejo sobre Inteligencia Artificial}\cite{ReglamentoIA2024}

    Este reglamento establece normas armonizadas para garantizar la seguridad, ética y transparencia en el desarrollo y aplicación de sistemas de IA en la Unión Europea.  
    En el contexto de este TFM, el sistema desarrollado se considera una herramienta de investigación y apoyo a la evaluación biomédica —no está concebido ni validado para toma de decisiones clínicas autónomas— 
    por lo que su uso actual no se presenta como IA de alto riesgo. No obstante, para alinearse con los requisitos se incorporan las siguientes medidas: documentación completa, evaluación de riesgos y validación, 
    trazabilidad y registro para una posterior reproducibilidad.
    
    \item \textbf{Real Decreto Legislativo 1/1996 sobre Propiedad Intelectual}\cite{RDL1996}
    
    En conformidad con el Real Decreto Legislativo 1/1996, el proyecto respeta la normativa sobre propiedad intelectual. Se ha optado por utilizar 
    únicamente código y herramientas de software libre y de código abierto para garantizar el cumplimiento de la normativa en materia de propiedad 
    intelectual.

\end{itemize}


\chapter{Metodología} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{metodologia}

En esta sección se describen de forma reproducible, las tres etapas clave experimentales implementadas de forma secuencial: 
\begin{itemize}
  \item \textbf{Fase 1:} Gestión y preprocesado del \textit{dataset}.
  \item \textbf{Fase 2:} Diseño experimental, entrenamiento y evaluación de modelos.
  \item \textbf{Fase 3:} Despliegue de la herramienta web.
\end{itemize}

\section{Gestión y preprocesado del \textit{dataset}}
\label{Gestión y preprocesado del dataset}

La base de cualquier sistema de aprendizaje de Inteligencia Artificial reside en la calidad de los datos. Esta primera fase, se centra en el estudio y procesamiento del conjunto de datos (\autoref{sec:Dataset}), que servirá como
fuente de verdad (\textit{ground truth}) en las etapas posteriores.

\begin{itemize}
  \item \textbf{Adquisición y consolidación:} El punto de partida es el conjunto de imágenes médicas proporcionado por Microptic S.L. \cite{microptic} 
  Se realiza un análisis para comprender su estructura, el formato original (PascVOC) y la calidad de las anotaciones. Un conjunto de datos para entrenamiento y tres para \textit{test}.
  \item \textbf{Validación por expertos:} Atendiendo a la revisión de feedback proporcionada por expertos sobre los conjuntos de \textit{test}, se procede con el reetiquetado (mediante LabelImage \cite{labelimg_github}) para asegurar una mejor precisión de las anotaciones. 
  Como no se tiene feedback respecto al conjunto de entrenamiento y, además, como se quiere comparar con la prueba de concepto inicial, no se modifica ni se pide una nueva valoración de expertos sobre el conjunto de datos de entrenamiento.
  \item \textbf{Estructuración del dataset:} Para entrenar las diferentes arquitecturas de YOLO, el conjunto de datos de entrenamiento original, se divide el conjunto de datos en entrenamiento (80\%) y validación (20\%). Empleando para esto, una semilla determinada para garantizar que los experimentos sean reproducibles. 
  \item \textbf{Nuevo formato de datos:} Una vez se ha organizado la estructura de carpetas, las anotaciones en formato PascVOC se convierten a formato YOLO, que es requerido para la metodología actual. Adicionalmente, se generó una versión en formato COCO para garantizar la interoperabilidad y facilitar futuras aplicaciones con otros marcos de trabajo.
\end{itemize}

\section{Diseño experimental, entrenamiento y evaluación de modelos.}
\label{Entrenamiento, validación y evaluación de modelos YOLO}

Esta fase constituye el núcleo de la investigación, donde se exploran diferentes arquitecturas y configuraciones para obtener el modelo óptimo.
El detalle completo previo y durante la etapa de entrenamiento de los diferentes modelos se recoge en la \autoref{sec:Configuraciones}.

\begin{itemize}
  \item \textbf{Arquitectura:} El código de esta fase se estructura de forma altamente modular, facilitando el mantenimiento y escalabilidad. La lógica completa se encapsula en clases y librerías reutilizables, manteniendo los cuadernos de Jupyter 
  (Jupyter Notebooks) limpios y con un hilo argumental claro; dando lugar a una estructura limpia y clara \cite{repoTFM}.
  \item \textbf{Optimización de modelos:} Se realiza una búsqueda sistemática mediante el framework Optuna \cite{Optuna} para la optimización automática de hiperparámetros.
  \item \textbf{Entrenamiento y validación:} Para obtener modelos que generalicen lo mejor posible, se emplean las técnicas:
  \begin{itemize}
    \item \textbf{Aumento de datos (\textit{data augmentation}):} Durante el proceso de entrenamiento, se aplican transformaciones para enriquecer la diversidad de los datos y reducir el \textit{overfitting}.
    \item \textbf{Validación cruzada (\textit{k-fold cross-validation}):} Se emplea para evaluar de forma robusta la capacidad de generalización de los modelos y la configuración de los hiperparámetros.
  \end{itemize} 
  \item \textbf{Exploración de enfoques avanzados:} Adicionalmente, se aborda el problema desde otras perspectivas:
  \begin{itemize}
    \item \textbf{Ensamblado de modelos:} Se combinan las predicciones de diferentes modelos individuales para crear un nuevo sistema de detección de células redondas. 
    \item \textbf{Modelo personalizado:} Se diseña una arquitectura de red neuronal propia. 
  \end{itemize}
  \item \textbf{Evaluación:} Se realiza una evaluación cuantitativa y cualitativa de los resultados (Ver \autoref{Resultados}).
  \begin{itemize}
    \item \textbf{Evaluación cuantitativa:} Se estudia el rendimiento individual de los modelos sobre los diferentes conjuntos de \textit{test}. Para esto, se emplean las métricas: 
    \textit{precision}, \textit{recall}, mAP@0.5, mAP@0.5:0.95, el tiempo de inferencia y el F1-confianza (\autoref{sec:Métricas}). 
    \item \textbf{Evaluación cualitativa:} Se evalúan los modelos en un entorno problemático, como son las imágenes con Anillos de Newton (\autoref{sec:Definición del problema}). Asimismo, se realiza una comparativa de resultados respecto algunas imágenes de resultados obtenidos en la prueba de concepto inicial. 
  \end{itemize}
\end{itemize}

\section{Desarrollo y despliegue de la herramienta web}
\label{Despliegue de la herramienta web}

La tercera y última fase del proyecto, es dar una solución tangible y accesible para los usuarios finales (expertos) (\autoref{Herramienta web}).

\begin{itemize}
  \item \textbf{Desarrollo de la interfaz:} Se define una aplicación interactiva empleando el framework de Streamlit, con el fin de que los expertos médicos puedan interactuar de forma intuitiva con el sistema de detección de células redondas.
  \item \textbf{Funcionalidades:} La prueba de concepto se integra con una serie de funcionalidades como la carga de imágenes y anotaciones asociadas (opcional), selección de modelo de inferencia e intervalo de confianza de las predicciones, 
  visualización de las detecciones, comparativa de resultados con el \textit{ground truth}, exportación de predicciones y métricas sobre un conjunto de imágenes. 
  \item \textbf{Arquitectura de software:} La aplicación se diseña con un enfoque modular, que separa la lógica de procesado de la interfaz de usuario, garantizando la escalabilidad y la facilidad de mantenimiento \cite{repoTFM}.
\end{itemize}

\chapter{Experimentación} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Experimentación}

\section{Dataset}
\label{sec:Dataset}
El dataset inicial proporcionado por la empresa MICROPTIC \cite{microptic} está constituido por un conjunto de \textit{train} y \textit{test} en formato PascalVOC.

\begin{table}[htbp]
\caption{Distribución del dataset original}
\centering
\begingroup
\setlength{\tabcolsep}{8pt}
\small
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{l c c c c c}
\toprule
Partición & Resoluciones (px) & Nº imágenes (resolución) & Nº imágenes & Sin instancias & Escala grises\\
\midrule
\textit{Train} & \makecell[l]{1280\,×\,1024 \\ 768\,×\,616} & \makecell[r]{356 \\ 17} & 373 & 23 & 3 canales\\ 
\arrayrulecolor{gray!30}\specialrule{0.6pt}{0pt}{0pt}\arrayrulecolor{black}
\textit{Test}  & \makecell[l]{1280\,×\,1024 \\ 768\,×\,616} & \makecell[r]{87 \\ 7}   & 94  & 6  & 3 canales\\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\endgroup
\label{tab:dataset_original}
\end{table}

El conjunto de datos inicial que nos proporciona Microptic \cite{microptic} está compuesto por 373 imágenes para \textit{train} y 94 para \textit{test}; de las cuales no presentan anotaciones: 23 imágenes de \textit{train} y 6 de \textit{test}.
El conjunto de entrenamiento se divide utilizando la función \texttt{train\_test\_split} de \textit{scikit-learn}, reservando el $80\%$ (298 imágenes) para entrenamiento y el $20\%$ (75 imágenes) para validación. 
Esta división se realiza de forma aleatoria pero reproducible, fijando la semilla (\texttt{random\_state = 42}) para garantizar la consistencia de los resultados.

Adicionalmente, la empresa proporciona un conjunto de datos del \textit{test} reevaluado por expertos del dominio. Este conjunto se reetiqueta utilizando la herramienta \textit{LabelImg} \cite{labelimg_github} en formato PascVOC. 
Las correcciones afectan a un total de 60 imágenes, incrementando el número de instancias de 1273 a 1412.

Asimismo, se incluyeron dos conjuntos adicionales denominados \textit{test2} y \textit{test3}, inicialmente sin anotaciones pero que contenían \textit{bounding boxes} generados por modelos YOLO \cite{ultralytics_models} preentrenados 
por el grupo GVIS de la Universidad de León. Estas predicciones fueron posteriormente corregidas y validadas por expertos, proporcionando dos conjuntos adicionales para evaluación. 
La composición final del dataset se presenta en la Tabla~\ref{tab:dataset_final}.

\clearpage
\begin{table}[htbp]
\caption{Resumen del dataset de actuación}
\centering
\rowcolors{2}{gray!12}{white}
\begin{tabular}{l c c c c c}
\toprule
Partición & Imágenes & Instancias & 1280$\times$1024 & 768$\times$616 & Escala grises\\
\midrule
\textit{Train}          & 298 & 3934 & 282 & 16 & 3 canales\\
\textit{Validation}     &  75 &  878 & 74  & 1  & 3 canales\\
\textit{Original\_test} &  94 & 1273 & 87  & 7  & 3 canales\\
\textit{Test}           &  94 & 1412 & 87  & 7  & 3 canales\\
\textit{Test2}          &  10 &  144 & 0   & 10 & 1 canal\\
\textit{Test3}          &  59 & 1135 & 56  & 3  & 1 canal\\
\bottomrule
\end{tabular}
\label{tab:dataset_final}
\end{table}

El conjunto de \textit{train}, \textit{validation} y \textit{test} están constituidos por imágenes RGB de 3 canales, mientras que las imágenes 
de los conjuntos de \textit{test2} y \textit{test3} están compuestos por imágenes en escala de grises de un único canal. 

Para el entrenamiento de diferentes arquitecturas de detección de objetos, se convierten los formatos de PascVOC a YOLO y YOLO a COCO. 
Para más deralle, la información relativa al preprocesamiento del dataset se encuentra en el documento \texttt{preprocesamiento.ipynb} del repositorio \cite{repoTFM}.

\section{Entorno de desarrollo}
\label{sec:Entorno de desarrollo}
Se utilizan dos entornos de desarrrollo complementarios, garantizando la reproducibilidad y escalabilidad de los resultados obtenidos.

\subsection{\textit{Hardware}}
\subsubsection{Hardware Local}
El entorno principal de desarrollo es un equipo \textit{Lenovo IdeaPad Gaming 3} con las siguientes especificaciones técnicas:

\begin{itemize}
    \item \textbf{Procesador}: AMD Ryzen 5 5600H with Radeon Graphics
    \begin{itemize}
        \item Velocidad base: 3,30 GHz
        \item Núcleos físicos: 6
        \item Procesadores lógicos: 12
        \item Caché L1: 384 kB
        \item Caché L2: 3,0 MB  
        \item Caché L3: 16,0 MB
        \item Virtualización: Habilitada
    \end{itemize}
    
    \item \textbf{GPU}: NVIDIA GeForce RTX 3050 Laptop GPU
    \begin{itemize}
        \item Memoria dedicada: 4,0 GB GDDR6
        \item Arquitectura: Ampere
        \item Soporte CUDA: 12.9
        \item Driver version: 576.02
    \end{itemize}
    
    \item \textbf{Memoria RAM}: 16 GB DDR4 SODIMM
    \begin{itemize}
        \item Velocidad: 3200 MHz
        \item Configuración: 2 módulos de 8 GB
    \end{itemize}
    
    \item \textbf{Almacenamiento}: SSD NVMe PCIe Gen3 x4
    \begin{itemize}
        \item Capacidad: 512 GB
        \item Modelo: Micron MTFDHBA512QFD
    \end{itemize}
\end{itemize}

\subsubsection{Servidor privado}
Como entorno complementario se ha empleado un servidor remoto proporcionado por del grupo GVIS de la Universidad de León.

\begin{itemize}
    \item \textbf{GPU}: Tesla T4 con 15 GB de memoria
    \item \textbf{RAM del sistema}: 12,7 GB
    \item \textbf{Almacenamiento temporal}: 78,2 GB SSD
\end{itemize}

\subsection{\textit{Software y Frameworks}}

El desarrollo se realizó empleando el siguiente \textit{stack} tecnológico:

\begin{itemize}
    \item \textbf{Sistema Operativo}: Windows 11
    \item \textbf{Entorno Python}: Miniconda3 (entorno \texttt{TFM})
    \item \textbf{IDE}: Microsoft Visual Studio Code
    \item \textbf{CUDA Toolkit}: Versión 12.9
    \item \textbf{Frameworks principales}:
    \begin{itemize}
        \item PyTorch 2.6.0 con soporte CUDA 12.6
        \item Ultralytics 8.3.177
        \item OpenCV para procesamiento de imágenes
        \item Optuna para optimización de hiperparámetros 
        \item Streamlit para desarrollo de interfaces web
        \item Pandas para análisis y manipulación de datos
        \item Matplotlib para visualización de datos 
        \item Scikit-learn para métricas y otras utilidades de \textit{machine learning}
        \item Jupyter Notebook para desarrollo y análisis interactivo
        \item linter ruff para mantener un código limpio, coherente y más fácil de mantener.
    \end{itemize}
\end{itemize}

Para el despliegue del proyecto se recomienda tener todas las dependencias del \texttt{requirements.txt} \cite{repoTFM}, el cual 
recoge todas las librerías y versiones necesarias para la correcta ejecución del entorno.

\section{Configuraciones}
\label{sec:Configuraciones}

\subsection{Entrenamiento}

Aquí tengo que poner una tabla con las configuraciones de entrenamiento para ser recreado y cumplir con las normas éticas.

\begin{table}[H]
\caption{Hiperparámetros principales de entrenamiento para cada modelo}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Modelo} & \textbf{Nº Trials} & \textbf{Epoch Optuna} & \textbf{Epoch Train} & \textbf{Batch} & \textbf{Image Size} \\
\hline
yolov8s   & 6  & 25 & 40 & 12 & 704 \\
yolov9s   & 6  & 25 & 40 & 10 & 704 \\
yolov10s  & 6  & 25 & 40 & 10 & 704 \\
yolov11s  & 6  & 25 & 40 & 10 & 704 \\
yolov12s  & 6  & 25 & 40 & 7 & 704 \\
yolov12x  & 5  & 25 & 40 & 7 & 704 \\
custom    & 10 & 25 & 40 & 12 & 704 \\
\hline
\end{tabular}
\end{table}


\begin{table}[H]
\caption{Principales hiperparámetros óptimos seleccionados para cada modelo.}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Modelo} & \textbf{lr0} & \textbf{lrf} & \textbf{momentum} & \textbf{weight\_decay} & \textbf{optimizer} \\
\hline
yolov8s   & 0.00540 & 0.00399 & 0.90621 & 0.00010 & SGD \\ \hline
yolov9s   & 0.00023 & 0.00153 & 0.84564 & 0.00033 & AdamW \\
yolov10s  & 0.00540 & 0.00399 & 0.90621 & 0.00010 & SGD \\
yolov11s  & 0.00023 & 0.00153 & 0.84564 & 0.00033 & AdamW \\
yolov12s  & 0.00023 & 0.00932 & 0.91627 & 0.00087 & AdamW \\
custom    & 0.00153 & 0.00111 & 0.89113 & 0.00015 & AdamW \\
\hline
\end{tabular}
\end{table}

Hiperparámetros del data augmentation:

\begin{description}
  \item[\textbf{warmup\_epochs}:] 5
  \item[\textbf{warmup\_momentum}:] 0.75
  \item[\textbf{degrees}:] 45
  \item[\textbf{translate}:] 0.1
  \item[\textbf{scale}:] 0.06
  \item[\textbf{flipud}:] 0.5
  \item[\textbf{fliplr}:] 0.5
  \item[\textbf{mosaic}:] 0
  \item[\textbf{close\_mosaic}:] 0
\end{description}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/yolo_plots/map50-95.png}
  \caption{mAP@0.5:0.95 durante el proceso de entrenamiento.}
  \label{fig:yolo_train_map95}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/yolo_plots/map50.png}
  \caption{mAP@0.5 durante el proceso de entrenamiento.}
  \label{fig:yolo_train_map50}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/yolo_plots/precision.png}
  \caption{\textit{Precision} del modelo sobre el conjunto de validación.}
  \label{fig:yolo_train_precision}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/yolo_plots/recall.png}
  \caption{\textit{Recall} del modelo sobre el conjunto de validación.}
  \label{fig:yolo_train_recall}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/yolo_plots/box_loss.png}
  \caption{\textit{Box Loss} sobre el conjunto de entrenamiento y validación.}
  \label{fig:yolo_train_box_loss}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/yolo_plots/cls_loss.png}
  \caption{\textit{Clasification Loss} sobre el conjunto de entrenamiento y validación.}
  \label{fig:yolo_train_cls_loss}
\end{figure}

\subsection{Validación cruzada}
\subsection{Ensamblado de modelos}
\subsection{Modelo personalizado}

\section{Métricas}
\label{sec:Métricas}
Para la evaluación de los modelos, se emplea un conjunto de métricas fundamentales que se exponen a continuación.

\begin{description}
  \item[\textit{Precision}] Mide la proporción de detecciones correctas entre todas las detecciones realizadas:
  \[
    \mathrm{Precision} = \frac{TP}{TP + FP}
  \]
  donde TP son los verdaderos positivos y FP los falsos positivos. Alta precision indica pocas detecciones erróneas.

  \item[\textit{Recall}]: Mide la proporción de instancias reales que han sido detectadas:
  \[
    \mathrm{Recall} = \frac{TP}{TP + FN}
  \]
  donde FN son los falsos negativos. Un recall alto indica que el modelo encuentra la mayoría de las instancias reales.

  \item[\textit{F1-score}] Es la media armónica entre la precisión y el recall, proporcionando una medida única que equilibra ambos aspectos. Se calcula como:
  \[
    \mathrm{F1} = 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}
  \]
  El F1-score es especialmente útil cuando es importante considerar tanto los falsos positivos como los falsos negativos. En este trabajo se utiliza como métrica principal en las curvas F1-confianza y en la herramienta web.

  \item[\textit{mean Average Precision (mAP)}] Para detección de objetos se calcula la curva precision–recall para cada clase y su área bajo la curva (AP). El \textit{mean Average Precision} es la media de las AP sobre todas las clases:
  \[
    \mathrm{mAP} = \frac{1}{C}\sum_{c=1}^{C} \mathrm{AP}_c
  \]
  donde $C$ es el número de clases. En detección se considera una predicción como verdadero positivo si el \textit{Intersection over Union} (IoU) entre la caja predicha y la caja ground truth supera un umbral (por ejemplo IoU $\geq 0.5$). 

  \medskip

  \noindent\textbf{mAP@0.5 (mAP50).} Es la mAP calculada usando un único umbral de IoU igual a 0.5. Formalmente:
  \[
    \mathrm{mAP@0.5} \;=\; \frac{1}{C}\sum_{c=1}^{C} \mathrm{AP}_c(\mathrm{IoU}=0.5)
  \]
  Es una medida menos exigente, que acepta solapamientos moderados entre predicción y \textit{ground truth}.

  \medskip

  \noindent\textbf{mAP@[0.5:0.95] (mAP50:95).} Es la mAP estándar del benchmark COCO que promedia la AP de cada clase sobre múltiples umbrales de IoU desde 0.50 hasta 0.95 con paso 0.05 (10 umbrales: 0.50, 0.55, …, 0.95). Nota: COCO calcula cada AP integrando la curva precision–recall muestreada en 101 puntos, y después se promedian las AP en los 10 umbrales para obtener mAP@[0.5:0.95].
  \[
    \mathrm{mAP}_{[0.5:0.95]} \;=\; \frac{1}{C}\frac{1}{T}\sum_{c=1}^{C}\sum_{t\in\{0.50,0.55,\dots,0.95\}} \mathrm{AP}_c(\mathrm{IoU}=t)
  \]
  donde $T=10$. Esta métrica es más exigente porque penaliza detecciones con IoU bajos y refleja mejor la precisión espacial del modelo.

  \item[Inferencia (ms)] Tiempo medio (en milisegundos) que tarda un modelo en procesar una única imagen (inferencia). Métrica relevante para proyectos en tiempo real.

\end{description}
\arrayrulecolor[HTML]{B9DAE1}


\chapter{Resultados} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Resultados}

\section{Resultados cuantitativos}
\label{sec:Resultados cuantitativos}

\begin{table}[H]
  \caption{Resultados de los modelos en la prueba de concepto inicial}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccccc}
      \hline
      \textbf{Modelo} & \textbf{Precisión (P)} & \textbf{Recall (R)} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} & \textbf{Inferencia (ms)} \\
      \hline
      YOLOv7      & 0.87  & \textbf{0.889} & \textbf{0.935} & \textbf{0.722} & 20.0   \\
      YOLOv7-W6   & 0.847 & 0.883 & 0.925 & 0.694 & \textbf{16.0}   \\
      YOLOv7-E6E  & \textbf{0.906} & 0.857 & 0.934 & 0.721 & 127.5  \\
      \hline
  \end{tabular}
  }
  \label{tab:yolov7_results}
\end{table}

\begin{table}[H]
  \caption{Resultados de los modelos sobre el conjunto de \textit{test} original}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{lccccc}
      \hline
      \textbf{Modelo} & \textbf{Precisión (P)} & \textbf{Recall (R)} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} & \textbf{Inferencia (ms)} \\
      \hline
      yolov8s    & 0.843 & 0.844 & 0.920 & 0.722 & \textbf{11.006} \\
      yolov9s    & 0.838 & 0.852 & 0.921 & 0.721 & 13.485 \\
      yolov10s   & 0.821 & \textbf{0.867} & 0.918 & 0.717 & 11.221 \\
      yolov11s   & \textbf{0.861} & 0.824 & 0.921 & 0.732 & 11.248 \\
      yolov12s   & 0.856 & 0.863 & \textbf{0.936} & \textbf{0.76} & 14.615 \\
      yolov12l   & 0.856 & 0.836 & 0.921 & 0.729 & 48.146 \\
      custom     & 0.835 & 0.838 & 0.909 & 0.695 & 13.058 \\
      ensemble   & 0.815 & 0.895 & 0.864 & 0.697 & 75.073 \\
      \hline
  \end{tabular}
  }
  \label{tab:yolov8_12_results}
\end{table}

% Tabla para los resultados sobre los conjuntos de Test
\definecolor{turquoise}{RGB}{64,224,208}
\begin{table}[ht]
\caption{Evaluación de modelos sobre los nuevos conjuntos de \textit{test}}
\centering
\rowcolors{2}{gray!15}{white}
\renewcommand{\arraystretch}{1.3}
\setlength{\arrayrulewidth}{1.2pt}
\resizebox{\textwidth}{!}{
\arrayrulecolor{gray}
\begin{tabular}{!{\vrule width 1.2pt}l|c|c|c|c|c|c!{\vrule width 1.2pt}}
\arrayrulecolor{black}
\specialrule{1.5pt}{0pt}{0pt}
\rowcolor{gray!30}
\textbf{Modelo} & \textbf{Test} & \textbf{Precisión} & \textbf{Recall} & \textbf{mAP@0.5} & \textbf{mAP@0.5:0.95} & \textbf{Inferencia (ms)} \\
\specialrule{1.2pt}{0pt}{0pt}
\arrayrulecolor{gray}
yolov8s   & test  & 0.855 & 0.838 & 0.926 & 0.704 & 11.844 \\ \hline
yolov9s   & test  & 0.851 & 0.876 & 0.933 & 0.706 & 13.492 \\ \hline
yolov10s  & test  & 0.851 & 0.862 & 0.927 & 0.701 & 11.120 \\ \hline
yolov11s  & test  & 0.846 & 0.866 & 0.930 & 0.716 & 12.143 \\ \hline
yolov12s  & test  & 0.862 & 0.848 & \cellcolor{turquoise!30}\textbf{0.936} & \cellcolor{turquoise!30}\textbf{0.740} & 14.519 \\ \hline
yolov12l  & test  & 0.846 & \cellcolor{turquoise!30}\textbf{0.889} & 0.934 & 0.714 & 45.214 \\ \hline
custom    & test  & 0.845 & 0.853 & 0.919 & 0.678 & 12.913 \\ \hline
ensemble & test & \cellcolor{turquoise!30}\textbf{0.869} & 0.860 & 0.842 & 0.664 & 69.572 \\
\arrayrulecolor{black}
\specialrule{1.2pt}{0pt}{0pt}
\arrayrulecolor{gray}
yolov8s   & test2 & 0.924 & 0.938 & 0.969 & 0.553 & 30.901 \\ \hline
yolov9s   & test2 & 0.917 & 0.927 & 0.961 & 0.542 & 46.502 \\ \hline
yolov10s  & test2 & 0.956 & 0.898 & 0.962 & \cellcolor{turquoise!30}\textbf{0.589} & 34.355 \\ \hline
yolov11s  & test2 & 0.929 & 0.917 & 0.964 & 0.534 & 33.114 \\ \hline
yolov12s  & test2 & \cellcolor{turquoise!30}\textbf{0.965} & 0.917 & \cellcolor{turquoise!30}\textbf{0.975} & 0.556 & 51.508 \\ \hline
yolov12l  & test2 & 0.957 & 0.926 & \cellcolor{turquoise!30}\textbf{0.975} & 0.562 & 75.871 \\ \hline
custom    & test2 & 0.945 & 0.903 & 0.957 & 0.539 & 35.626 \\ \hline
ensemble & test2 & 0.919 & \cellcolor{turquoise!30}\textbf{0.944} & 0.935 & 0.509 & 54.958 \\
\arrayrulecolor{black}
\specialrule{1.2pt}{0pt}{0pt}
\arrayrulecolor{gray}
yolov8s   & test3 & 0.842 & 0.862 & 0.937 & 0.700 & 12.815 \\ \hline
yolov9s   & test3 & 0.843 & 0.870 & 0.935 & 0.680 & 14.772 \\ \hline
yolov10s  & test3 & \cellcolor{turquoise!30}\textbf{0.873} & 0.826 & 0.926 & 0.668 & 11.230 \\ \hline
yolov11s  & test3 & 0.855 & 0.874 & 0.935 & 0.698 & 10.875 \\ \hline
yolov12s  & test3 & 0.836 & 0.821 & 0.922 & \cellcolor{turquoise!30}\textbf{0.723} & 15.439 \\ \hline
yolov12l  & test3 & 0.849 & \cellcolor{turquoise!30}\textbf{0.894} & \cellcolor{turquoise!30}\textbf{0.942} & 0.701 & 47.256 \\ \hline
custom    & test3 & \cellcolor{turquoise!30}\textbf{0.873} & 0.843 & 0.928 & 0.660 & 11.373 \\ \hline
ensemble & test3 & \cellcolor{turquoise!30}\textbf{0.873} & 0.857 & 0.837 & 0.663 & 63.758 \\
\arrayrulecolor{black}
\specialrule{1.5pt}{0pt}{0pt}
\end{tabular}
}
\end{table}

Es importante que hablemos de esta problemática y como evitamos un reetiquetado de imágenes que pueden dar más confusión al confundir clases, y continuar la tendencia monoclase.

\clearpage
\section{Resultados cualitativos}
\label{Resultados cualitativos}


% Tabla para los artefactos
\definecolor{mygreen}{RGB}{0,180,0}
\definecolor{myred}{RGB}{220,0,0}
\definecolor{mygray}{gray}{0.7}
\setlength{\arrayrulewidth}{1.2pt} % Grosor general de líneas (verticales y horizontales negras)
\begin{table}[ht]
\caption{Evaluación de modelos sobre imágenes con artefactos}
\centering 
\resizebox{\textwidth}{!}{%
\begin{tabular}{!{\vrule width 1.2pt}c|c|c|c|c|c|c|c|c|c!{\vrule width 1.2pt}}
\arrayrulecolor{black}\hline
\textbf{Modelo/Umbral} & \textbf{59 imagen} & \textbf{61 imagen} & \textbf{219 imagen} & \textbf{369 imagen} & \textbf{already tested-00} & \textbf{already tested-15} & \textbf{already tested-16} & \textbf{already tested-17} & \textbf{already tested-22} \\
\arrayrulecolor{black}\hline
Test              & 1 & 1 & 1 & 1 & 3 & 3 & 3 & 3 & 3 \\
\arrayrulecolor{black}\hline
yolov8s 0.5       & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{mygray}\hline
yolov8s 0.7       & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{black}\hline
yolov9s 0.5       & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{mygray}\hline
yolov9s 0.7       & \cellcolor{mygreen!30}\ding{51} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{black}\hline
yolov10s 0.5      & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{myred!30}\ding{55} \\
\arrayrulecolor{mygray}\hline
yolov10s 0.7      & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{black}\hline
yolov11s 0.5      & \cellcolor{mygreen!30}\ding{51} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{myred!30}\ding{55} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{mygray}\hline
yolov11s 0.7      & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{black}\hline
yolov12s 0.5      & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{mygray}\hline
yolov12s 0.7      & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} & \cellcolor{mygreen!30}\ding{51} \\
\arrayrulecolor{black}\hline
yolov12l 0.5      &  &  &  &  &  &  &  &  &  \\
\arrayrulecolor{mygray}\hline
yolov12l 0.7      &  &  &  &  &  &  &  &  &  \\
\arrayrulecolor{black}\hline
custom 0.5        &  &  &  &  &  &  &  &  &  \\
\arrayrulecolor{mygray}\hline
custom 0.7        &  &  &  &  &  &  &  &  &  \\
\arrayrulecolor{black}\hline
ensemble 0.5      &  &  &  &  &  &  &  &  &  \\
\arrayrulecolor{mygray}\hline
ensemble 0.7      &  &  &  &  &  &  &  &  &  \\
\arrayrulecolor{black}\hline
\end{tabular}
}
\end{table}

\chapter{Herramienta Web} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Herramienta web}
Con el propósito de definir una herramienta de análisis automatizado, se desarrolla una interfaz intuitiva e interactiva para la detección 
de células redondas sobre una muetra individual o colectiva. La información relativa a la aplicación,
se puede encontrar en el manual de usuario (\ref{Manual de usuario}).
\section{Objetivos de la herramienta}
\label{sec:Objetivos de la herramienta}

La herramienta web para la detección de células redondas tiene como principales objetivos:

\begin{itemize}
  \item{\textbf{Detección atomática de células en imágenes microscópicas en el contexto médico abordado con anterioridad:} Permitir identificar
  y localizar células redondas dotando al usuario de diferentes arquitecturas de modelos.}
  \item{\textbf{Comparación de rendimiento entre modelos:} Facilitar la evaluación comparativa entre diferentes modelos,
  permitiendo al usuario seleccionar el mejor modelo y el intervalo de actuación para el IoU.}
  \item{\textbf{Visualización de resultados:} Proporcionar una interface de visualización clara e intuitiva que permita al usuario 
  visualizar las predicciones del modelo y las \textit{bounding boxes} del \textit{ground truth}si se dispone de las mismas.}
  \item{\textbf{Análisis cuantitativo:} Permitir obtener métricas y estadísticas como: \textit{precision, accuracy}, conteo o IoU promedio}.
  \item{\textbf{Soporte a la investigación biomédica:} Facilitar la identificación temprana de diferentes grados de patología.} 
\end{itemize}

\section{Arquitectura}
\label{sec:Arquitectura}
La arquitectura de la aplicación sigue un diseño modular y escalable, basada en \textit{Streamlit}. Esto permite tener una aplicación monolítica que encapsula toda la funcionalidad en un único ejecutable y mantiene 
una clara separación entre la lógica de procesamiento y de presentación.

\subsection{Organización del proyecto}
La estructura principal es:
\begin{itemize}
  \item \texttt{app.py}: orquestación de la interfaz (widgets Streamlit), gestión de sesión y del flujo de inferencia.
  \item \texttt{utils/utils.py}: servicios de \textit{backend} lógico (carga de modelos, preprocesado, inferencia, postprocesado, visualización).
  \item \texttt{assets/styles.css}: estilo e integración visual mediante \texttt{st.markdown}.
  \item \texttt{models/\{final\_model\_yolovXs\}/}: modelos entrenados para la casuística.
  \item \texttt{runs/detect/}: salidas temporales de inferencia (imágenes anotadas, JSON/CSV).
\end{itemize}

Para más información, revisar la documentación \texttt{04.Code/cell\_detection\_App} en el repositorio del proyecto \cite{repoTFM}.

\subsection{Capas funcionales en Streamlit}
\begin{enumerate}
  \item \textbf{Interfaz y orquestación (UI)}: componentes como \texttt{st.file\_uploader}, \texttt{st.selectbox}, \texttt{st.slider}, \texttt{st.sidebar} y \texttt{st.image} permiten la interacción con el usuario de una forma intuitiva.
  \item \textbf{Servicios de modelo e inferencia}: Funciones en \texttt{utils/utils.py} para cargar pesos YOLO, seleccionar dispositivo (CPU/GPU), normalizar entradas y ejecutar el modelo sobre las entradas preprocesadas.
  \item \textbf{Pipeline de datos}: Preprocesado (lectura, redimensionado, normalización), postprocesado (NMS, filtrado por confianza, conversión a anotaciones PascalVOC), y renderizado de \textit{bounding boxes}. 
  Además, incluye la exportación de las prediciones en formato PascVOC. 
  \item \textbf{Estado y caché}: \texttt{st.session\_state} para parámetros y resultados interactivos; \texttt{@st.cache\_resource} evita recargar 
  pesos al cambiar sólo parámetros de inferencia; \texttt{@st.cache\_data} para memoizar resultados derivado (tablas/figuras).
  \item \textbf{Estilos y experiencia de usuario}: \texttt{assets/styles.css} para mejorar el aspecto visual que por defecto ofrece streamlit y uso de layout 
  responsivo (columnas, botones, tooltips) para mejorar la usabilidad.
  \item \textbf{Manejo de errores}: \texttt{st.info} para informar de estados (dispositivo, imágener y xml subidos), \texttt{st.warning} para advertir de un error en la lectura del xml, 
  \texttt{st.error} implide continuar con la ejecución si no encuentra modelo o al no ser capaz de procesar una imagen.
\end{enumerate}

\subsection{Flujo de ejecución}
\begin{enumerate}
  \item Inicio: se inicializa la sesión y se aplica el estilo personalizado CSS.
  \item El usuario selecciona modelo e IoU: Yolov12s y 0,5.
  \item Carga de modelo (una sola vez) vía \texttt{@st.cache\_resource}.
  \item Carga de imagen (individual o lote) con \texttt{st.file\_uploader}. Opcional: cargar el xml con las anotaciones en formato PascVOC.
  \item Preprocesado de la imagen y envío al dispositivo (CPU/GPU).
  \item Inferencia YOLO y postprocesado (NMS, métricas básicas).
  \item Visualización: imagen anotada, conteo, tablas, métricas y \textit{bounding boxes}.
\end{enumerate}


\chapter{Conclusión} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
hacer una evaluación de riesgos.
Por lo tanto, este Trabajo de Fin de Máster no es un ejercicio puramente académico, sino que se constituye como una Prueba de Concepto (PoC) con un objetivo industrial definido y un respaldo institucional estratégico.
\section{Conclusión general}
\label{sec:Conclusión general}
\section{Limitaciones del estudio}
\label{sec:Limitaciones del estudio}
\section{Lineas de trabajo futuro}
\label{sec:Lineas de trabajo futuro}

\subsection*{Aportaciones realizadas}

\subsection*{Problemas encontrados}

\section*{Opiniones personales}
\label{sec:Opiniones personales}

\subsection*{Trabajos futuros}
\subsection*{Agradecimientos}


%%%% BIBLIOGRAFÍA %%%%
\renewcommand\bibname{Bibliografía}

\bibliographystyle{unsrt} % Fichero con el formato de la bibliografía.
\nocite{*}
\bibliography{referencias}
%\bibliographystyle{unsrt} %plain %apalike

%%%% ANEXOS %%%%
\renewcommand{\appendixname}{Anexo}
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries}
  {Anexo \thechapter}{0pt}{\Huge}
\appendix

Los propios autores admiten que, en algunos casos, incluso para un experto es difícil distinguir si un objeto es una impureza o un espermatozoide. Esto introduce ambigüedad en los datos de entrenamiento, lo que perjudica el aprendizaje del modelo, especialmente para la clase más difícil ("impurezas") [Chen2024].

\chapter{Control de versiones} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Control de versiones}

En el marco de desarrollo del Trabajo de Fin de Máster (TFM), se ha empleado GitHub como servicio de control de versiones para 
gestionar eficientemente el código fuente y la documentación del proyecto; facilitando el seguimiento, la trazabilidad y la colaboración.

Esta herramienta permite un seguimiento del proyecto por parte del responsable o tutor. Además, de ser necesario, GitHub presenta
una funcionalidad para desarrolladores que permite editar el proyecto desde un navegador web. Facilitando la implementación de mejoras y la 
corrección de errores desde cualquier dispositivo con acceso a internet.

Casi la totalidad del desarrollo de este proyecto final lo podemos encontrar en el repositorio personal \cite{repoTFM}, cumpliendo en todo momento, con la normativa y legislación bajo las que se enmarca el proyecto.


\chapter{Seguimiento del proyecto} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Seguimiento de proyecto}

%Obligatorio. Seguimiento del trabajo real.


\chapter{Herramienta Web} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Herramienta Web anexo}

\section{Manual de usuario}
\label{Manual de usuario}
Para facilitar la interacción del usuario con la herramienta web vista con anterioridad, se define la siguiente guía de usuario.

Desde el entorno con la dependencia \textit{Streamlit}, se ejecuta el comando \texttt{streamlit run app.py}  se lanza la aplicación en el navegador predeterminado
como se muestra en la siguiente imagen.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/app/cell_app.png}
  \caption{Interfaz de la aplicación web.}
  \label{fig:cell_app}
\end{figure}

En este punto, el usuario puede tomar la decisión de procesar una imagen individual o un conjunto de imágenes. Para esto, 
tenemos el \textit{widget} que permite la selección entre "Detección individual" o "Carpeta completa". En ambos casos,
la aplicación nos muestra una guia básica de funcionamiento como se puede apreciar en el apartado "Introducciones" en la Figura \ref{fig:cell_app}.
De manera totalmente opcional, el usuario puede cargar junto a la imagen a estudio, el correspondiente conjunto de anotaciones.

El \textit{widget} de Configuración permite la seleccion manual del modelo a través de un desplazable, el umbral de confianza de IoU, 
la representación \textit{ground truth} (siempre que exista el xml asociado a la imagen) mediante un \textit{checkbox} (Figura \ref{fig:cell_app_gt}) y realizar un análisis exploratorio
sobre la imagen con un zoom (Figura \ref{fig:cell_app_zoom}). Asimismo, muestra el dispositivo (CPU,GPU) con el que se procesan las imágenes.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/app/prueba_imagen.png}
  \caption{Herramienta web: \textit{checkbox ground truth}.}
  \label{fig:cell_app_gt}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/app/zoom_imagen.png}
  \caption{Herramienta web: \textit{checkbox zoom}.}
  \label{fig:cell_app_zoom}
\end{figure}

Cabe destacar el \textit{widget} "Descargar XML" que permite al usuario descargar las prediciones del modelo realizadas sobre la imagen de entrada en un formato
PascalVOC. 

Si el usuario prefiere evaluar un conjunto de imágenes con sus respectivas anotaciones, se debe seleccionar la elección de "carpeta completa" como se muestra en la Figura \ref{fig:cell_app_directory}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/app/cell_app_directorio.png}
  \caption{Herramienta web para procesar un conjunto de imágenes.}
  \label{fig:cell_app_directory}
\end{figure}

En este modo de evaluación, no está permitido el uso de los \textit{widgets}: \textit{ground truth} y \textit{zoom}. Sin embargo, 
el procesamiento del modelo seleccionado junto con su correspondiente umbral de procesamiento de IoU, está acompañado de un pequeño conjunto de métricas:
\textit{precision}, \textit{recall} y \textit{mAP}. Para una mejor evaluación, la herramienta muestra una matriz de confusión (Figura \ref{fig:cell_app_metrics}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{figuras/app/matriz_confusión.png}
  \caption{Herramienta web: métricas y matriz de confusión.}
  \label{fig:cell_app_metrics}
\end{figure}

\section{Recomendaciones de uso}
\label{sec:Recomendaciones de uso}
A continuación, se recogen pautas prácticas para el uso correcto funcionamiento de la herramienta web.

\begin{itemize}
  \item \textbf{Entorno recomendado:} se recomienda usar un entorno conda para tener aisladas las dependencias del proyecto y evitar así, conflictos entre dependencias. 
  Desde el directorio \texttt{04.Code} se proponen dos alternativas:
    \begin{itemize}
      \item Instalación del entorno completo con conda (Recomendado):
        \begin{itemize}
          \item \texttt{conda env create -f environment.yml}
          \item \texttt{conda activate tfm\_env}
        \end{itemize}
      \item Instalación de un subconjunto de dependencias con conda:
        \begin{itemize}
          \item \texttt{conda create -n tfm\_env python=3.11 -y}
          \item \texttt{conda activate tfm\_env}
          \item \texttt{pip install -r 04.Codigo/requirements.txt}
        \end{itemize}
    \end{itemize}
  \item \textbf{Ejecución de la aplicación:} en la carpeta \texttt{04.Codigo/cell\_detection\_App} ejecutar:
    \begin{itemize}
      \item \texttt{streamlit run app.py}
      \item Automáticamente se abre en el navegador por defecto la URL que Streamlit indique (por defecto \texttt{http://localhost:8501}).
    \end{itemize}
  \item \textbf{Compatibilidad con modelos:} limitada a modelos de YOLO \cite{ultralytics_models}. 
\end{itemize}

\section{Resolución de problemas}
\label{sec:Resolución de problemas}
Posibles problemas que pueden tener lugar durante la instalación:

\begin{itemize}
  \item \textbf{La app no arranca / Streamlit muestra error:}
    \begin{itemize}
      \item Verificar dependencias: \texttt{pip install -r 04.Codigo/requirements.txt}
      \item Comprobar la salida en el terminal donde se ejecuta \texttt{streamlit run app.py} y revisar trazas de error.
      \item Borrar caché de Streamlit: \texttt{streamlit cache clear}.
    \end{itemize}
  \item \textbf{Modelo no encontrado o error de carga de pesos:}
    \begin{itemize}
      \item Confirmar que el fichero de pesos existe en \texttt{models/} y que la ruta es correcta (\path{04.Codigo/cell_detection_App/models/}).
    \end{itemize}
  \item \textbf{Problemas con GPU / CUDA:}
    \begin{itemize}
      \item Comprobar estado de la GPU: ejecutar \texttt{nvidia-smi} en terminal.
      \item Verificar que PyTorch detecta la GPU: 
        \begin{itemize}
          \item \texttt{python -c "import torch; print(torch.cuda.is\_available())"}
        \end{itemize}
      \item Si falla, forzar ejecución en CPU desde la UI o variable de entorno: \texttt{CUDA\_VISIBLE\_DEVICES=""}.
    \end{itemize}
  \item \textbf{Errores al procesar imágenes / XML mal formados:}
    \begin{itemize}
      \item Validar que las imágenes están en formatos soportados (\texttt{.jpg, .jpeg, .png}) y que los XML cumplen con el formato PascalVOC.
      \item Revisar el log de parsing en \texttt{04.Codigo/cell\_detection\_App/utils/} y corregir los ficheros señalados.
    \end{itemize}
\end{itemize}


\chapter{Evaluación comparativa de arquitecturas} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Evaluación comparativa de arquitecturas}
\section{Matrices de confusión}
\label{sec:Matrice de confusión}
\section{Curvas F1-confianza}
\label{sec:Curvas F1-confianza}
\section{Resultados cualitativos}
\label{sec:Resultados cualitativos anexo}


\end{document}


% % Cambio de nombre del TFM
% ¿Usamos las mismas imágenes que evalua Eduardo y las comparamos con estos modelos también? En anexo o en resultados? 
% Anexo, añado las matrices de confusión de cada modelo para cada test? con un IoU de 0.5? 
% Anexo añado Optuna